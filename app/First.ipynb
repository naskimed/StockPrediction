{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf872d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_path = 'Data.py'\n",
    "os.system(f'python {script_path}')\n",
    "!python Data.py\n",
    "%run Data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a99fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import List\n",
    "from statistics import mean, median\n",
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from PortfolioClasses import *\n",
    "from Data import *\n",
    "import math\n",
    "    \n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"Custom Portfolio Environment that follows gym interface\"\"\"\n",
    "\n",
    "    def __init__(self, df, total_return_wanted, volatility_wanted, prices_df, prices_bmk, starting_holdings, simulation_dates, portfolio_manager_behaviour,  state_space, action_space):\n",
    "    # def __init__(self, df, total_return_wanted, volatility_wanted, prices_df, prices_bmk, starting_holdings, simulation_dates, portfolio_manager_behaviour, change_behaviour = False, make_plots = False, state_space, action_space, previous_state = [], model_name = \"\", mode = \"\", iteration = \"\", ):\n",
    "        # simulation days\n",
    "        self.simulation_days = 300\n",
    "        self.simulation_dates = simulation_dates\n",
    "        self.nb_days = len(self.simulation_dates[:self.simulation_days-1])\n",
    "        self.day = 0\n",
    "\n",
    "        # the input data\n",
    "        self.df = df\n",
    "        self.prices_df = prices_df\n",
    "        self.prices_bmk = prices_bmk\n",
    "        self.starting_holdings = starting_holdings\n",
    "        self.end_date = end_date=simulation_dates.max()\n",
    "        self.portfolio_manager_behaviour = portfolio_manager_behaviour\n",
    "        self.nav_data = None\n",
    "        self.perf_data = None\n",
    "\n",
    "        # for the model\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.action_space,))\n",
    "        # self.observation_space = spaces.Box(\n",
    "        #     low=-np.inf, high=np.inf, shape=(self.state_space,), dtype= 'float64'\n",
    "        # )\n",
    "        low_values = [-np.inf] * 6  # Replace with appropriate minimum values if needed\n",
    "        high_values = [np.inf] * 6  # Replace with appropriate maximum values if needed\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array(low_values, dtype=np.float32),\n",
    "            high=np.array(high_values, dtype=np.float32),\n",
    "        )\n",
    "        self.terminal = False\n",
    "        self.make_plots = False\n",
    "        self.change_behaviour = False\n",
    "        # self.print_verbosity = print_verbosity\n",
    "        # self.previous_state = previous_state\n",
    "        # self.model_name = model_name\n",
    "        # self.mode = mode\n",
    "\n",
    "        # initialize reward\n",
    "        self.total_return_wanted = total_return_wanted\n",
    "        self.volatility_wanted = volatility_wanted\n",
    "        \n",
    "        self.reward = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        self.total_return = 0\n",
    "        self.volatility = 0\n",
    "        self.maxdrawdown = 0\n",
    "        self.IR = 0\n",
    "        self.hit_ratio = 0\n",
    "        self.win_loss_ratio = 0\n",
    "        # self.portfolio = None\n",
    "\n",
    "        # memorize all the total balance change\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        # self.date_memory = [self._get_date()]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Run the simulation loop\n",
    "        if self.day < self.nb_days:\n",
    "            self.done = False\n",
    "        else:\n",
    "            self.done = True\n",
    "        \n",
    "        self.day += 1\n",
    "        self.portfolio.step()\n",
    "        portfolio_copy = copy.deepcopy(self.portfolio)\n",
    "\n",
    "        # Get the updated data for calculating the performances \n",
    "        data = portfolio_copy.datacollector.get_agent_vars_dataframe()\n",
    "        \n",
    "        # Get the performances data to generate the reward\n",
    "        self.total_return, self.volatility, self.maxdrawdown, self.IR, self.hit_ratio, self.win_loss_ratio = self._get_performance(portfolio_copy,data)\n",
    "        if math.isnan(self.volatility):\n",
    "            self.volatility = 0.0\n",
    "        if math.isnan(self.total_return):\n",
    "            self.total_return = 0.0\n",
    "        if math.isnan(self.maxdrawdown):\n",
    "            self.maxdrawdown = 0.0\n",
    "        if math.isnan(self.IR):\n",
    "            self.IR = 0.0\n",
    "        if math.isnan(self.hit_ratio):\n",
    "            self.hit_ratio = 0.0\n",
    "        if math.isnan(self.win_loss_ratio):\n",
    "            self.win_loss_ratio = 0.0\n",
    "        # print(self.total_return, self.volatility, self.maxdrawdown, self.IR, self.hit_ratio, self.win_loss_ratio)\n",
    "        #Calculate the reward\n",
    "        self.reward = self._get_reward()\n",
    "    \n",
    "        self.observation = np.array([np.float32(self.total_return), np.float32(self.volatility), np.float32(self.maxdrawdown), np.float32(self.IR), np.float32(self.hit_ratio), np.float32(self.win_loss_ratio)])\n",
    "        # self.observation[np.isnan(self.observation)] = 0.0\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        # print(\"Returned Observation Shape:\", self.observation.shape)\n",
    "        # print(\"Returned Observation Data Type:\", self.observation.dtype)\n",
    "        print(self.observation)\n",
    "\n",
    "\n",
    "\n",
    "        return self.observation, self.reward, self.done, False , info\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        # initiate state\n",
    "        self.portfolio = Portfolio(1, self.starting_holdings, self.portfolio_manager_behaviour, self.simulation_dates, self.prices_df, self.prices_bmk)\n",
    "        self.reward = np.float32(0.0)\n",
    "        self.day = np.float32(0.0)\n",
    "        self.total_return = np.float32(0.0)\n",
    "        self.volatility = np.float32(0.0)\n",
    "        self.maxdrawdown = np.float32(0.0)\n",
    "        self.IR = np.float32(0.0)\n",
    "        self.hit_ratio = np.float32(0.0)\n",
    "        self.win_loss_ratio = np.float32(0.0)\n",
    "        self.done = False\n",
    "\n",
    "        # self.iteration=self.iteration\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        # self.date_memory = [self._get_date()]\n",
    "\n",
    "        self.observation = np.array([self.total_return, self.volatility, self.maxdrawdown, self.IR, self.hit_ratio, self.win_loss_ratio])\n",
    "\n",
    "        self.episode += 1\n",
    "\n",
    "        self.info = {}\n",
    "\n",
    "        return self.observation, self.info\n",
    "\n",
    "    \n",
    "    # def render(self, mode='human'):\n",
    "    #     ...\n",
    "\n",
    "    # def close (self):\n",
    "    #     ...\n",
    "\n",
    "    # def _get_date(self):\n",
    "\n",
    "    #     if len(self.df.tic.unique()) > 1:\n",
    "    #         date = self.data.date.unique()[0]\n",
    "    #     else:\n",
    "    #         date = self.data.date\n",
    "    #     return date\n",
    "\n",
    "\n",
    "\n",
    "    def _get_performance(self, portfolio_copy, data):\n",
    "        \n",
    "        def calculateMetrics(data, percentBoolean):\n",
    "            avg = mean(data)\n",
    "            med = median(data)\n",
    "            minimum = min(data)\n",
    "            maximum = max(data)\n",
    "            if percentBoolean:\n",
    "                return toPercentAndRound([minimum, med, avg, maximum], True)\n",
    "            return toPercentAndRound([minimum, med, avg, maximum], False)\n",
    "\n",
    "\n",
    "        def toPercentAndRound(nums, percentBoolean):\n",
    "            if percentBoolean:\n",
    "                for i in range(len(nums)):\n",
    "                    nums[i] = round(nums[i] * 100, 2)\n",
    "            else:\n",
    "                for i in range(len(nums)):\n",
    "                    nums[i] = round(nums[i], 2)\n",
    "            return nums\n",
    "\n",
    "\n",
    "        def formatPercentages(nums):\n",
    "            for i in range(len(nums)):\n",
    "                    nums[i] = f\"{nums[i]}\"\n",
    "            return nums\n",
    "\n",
    "\n",
    "        def Volatility(data):\n",
    "            return data.std()*math.sqrt(255)\n",
    "\n",
    "        def IR(data):\n",
    "            stdev=data.std()*math.sqrt(255)\n",
    "            perf=data.sum()\n",
    "            return (perf/stdev)\n",
    " \n",
    "\n",
    "        def MaxDrawdown(data):\n",
    "            dd_series=data.cumsum() - data.cumsum().cummax()\n",
    "            max_dd=dd_series.min()\n",
    "            return(max_dd)\n",
    "\n",
    "        self.nav_data = pd.pivot_table(data[~data['date'].isna()], index=['date'], columns=data.index.get_level_values('AgentID'), values='NAV')\n",
    "        self.perf_data = data[~data['date'].isna()]\n",
    "        perf = pd.pivot_table(self.perf_data, index=['date'], columns=self.perf_data.index.get_level_values('AgentID'), values='performance')\n",
    "        gg_perf = perf.diff(-1).agg(['count','sum','std',IR, MaxDrawdown, Volatility]).transpose()\n",
    "\n",
    "        most_recent_date = self.perf_data['date'].max()\n",
    "        most_recent_perf = perf[perf.index == most_recent_date]\n",
    "        totalReturn = most_recent_perf.iloc[-1]\n",
    "\n",
    "        all_ptfs = [obj for obj in portfolio_copy.schedule.agents if ((isinstance(obj, PortfolioManager)))]\n",
    "        cols=[\"pft_id\",\"number_bets\", 'nav', \"performance\", \"hit_ratio\",\"win_loss_ratio\"]\n",
    "        behave=pd.DataFrame([],columns=cols)\n",
    "\n",
    "        hitRatioData = []\n",
    "        winLossRatioData = []\n",
    "        i=0\n",
    "\n",
    "        for ptf in all_ptfs:\n",
    "            bets=ptf.bets['closed'].copy()\n",
    "            still_active_bets=ptf.bets['active']\n",
    "            still_active_bets['security_id']=ptf.bets['active'].index\n",
    "            still_active_bets['end_date']=end_date\n",
    "            still_active_bets['next_decision']=\"Still Alive\"\n",
    "            bets=bets._append(still_active_bets, ignore_index=True)\n",
    "            bets = bets.query(\"security_id != 'Cash'\")\n",
    "            hit_ratio=len(bets[bets['performance']>0])/len(bets)\n",
    "            win_loss_ratio=-bets[bets['performance']>0]['performance'].mean() /bets[bets['performance']<0]['performance'].mean()\n",
    "            tab=[ptf.unique_id,len(bets),ptf.nav, ptf.performance,  hit_ratio,win_loss_ratio]\n",
    "            behave=behave._append(pd.DataFrame([tab],columns=cols), ignore_index=True)\n",
    "\n",
    "            winLossRatioData.append(win_loss_ratio)\n",
    "            hitRatioData.append(hit_ratio)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        Metrics = [\"Min\", \"Median\", \"Average\", \"Max\"]\n",
    "\n",
    "        total_return = formatPercentages(calculateMetrics(totalReturn, True))[3]\n",
    "        volatility = formatPercentages(calculateMetrics(gg_perf['Volatility'], True))[3]\n",
    "        IR = formatPercentages(calculateMetrics(gg_perf['IR'], True))[3]\n",
    "        maxdrawdown = formatPercentages(calculateMetrics(gg_perf['MaxDrawdown'], True))[3]\n",
    "        hit_ratio = calculateMetrics(hitRatioData, False)[3]\n",
    "        win_loss_ratio = calculateMetrics(winLossRatioData, False)[3]\n",
    "\n",
    "        return float(total_return), float(volatility), float(maxdrawdown), float(IR), float(hit_ratio), float(win_loss_ratio) \n",
    "\n",
    "\n",
    "\n",
    "    def _get_reward(self):\n",
    "\n",
    "        # Defining the weights \n",
    "        w1 = 0.5  \n",
    "        w2 = 0.2  \n",
    "        w3 = 0.1  \n",
    "        w4 = 0.1\n",
    "        w5 = 0.05\n",
    "        w6 = 0.05\n",
    "            \n",
    "        # Normalizing the values\n",
    "        normalized_total_return = self.total_return / 100 \n",
    "        normalized_volatility = self.volatility / 100\n",
    "        normalized_max_drawdown = self.maxdrawdown / 100\n",
    "        normalized_IR = self.IR / 100\n",
    "        normalized_hit_ratio = self.hit_ratio / 100\n",
    "        normalized_win_loss_ratio = self.win_loss_ratio / 100\n",
    "\n",
    "        # Calculate the reward \n",
    "        reward = w1 * normalized_total_return + w2 * normalized_volatility + w3 * normalized_max_drawdown + w4 * normalized_IR + w5 * normalized_hit_ratio + w6 * normalized_win_loss_ratio\n",
    "            \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "88271aa0-c035-4059-8fff-14036f7d9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95647fd6-9b95-4d3d-b531-995a720fdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnv(prices_df, 10, 10, prices_df, prices_bmk, starting_holdings, simulation_dates, portfolio_manager_behaviour, 6, 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1caacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioEnv(prices_df, 10, 10, prices_df, prices_bmk, starting_holdings, simulation_dates, portfolio_manager_behaviour, 6, 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4da0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    done = False \n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        random_action = env.action_space.sample()\n",
    "        print(\"action\", random_action)\n",
    "        obs, reward, done, false, info = env.step(random_action)\n",
    "        print('reward',reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a96509",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{int(time.time())}\"\n",
    "logdir = f\"logs/{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e2267",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "env = PortfolioEnv(prices_df, 10, 10, prices_df, prices_bmk, starting_holdings, simulation_dates, portfolio_manager_behaviour, 6, 9000)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose = 1, tensorboard = logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71399383",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 10000\n",
    "for i in range(1,100):\n",
    "    model.learn(total_timesteps = TIMESTEPS, reset_num_timesteps = False, tb_log_name = \"POO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
